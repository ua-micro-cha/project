# 26 July 2024

## humann step 1 scripts
<a href="https://huttenhower.sph.harvard.edu/humann/" target="_blank">HUMAnN 3.0</a>

- Open the directory containing slurm and .py scripts in the terminal.

- Rid hidden and unexpected characters from files:
'sed -i 's/\r$//' humann_step1.slurm'
'sed -i 's/\r$//' humann_step1.py'

- Submit the script using the `sbatch` command:
'sbatch humann_step1.slurm'

### humann_step1.slurm
``` slurm
#!/bin/bash
#SBATCH --job-name=humann_step1
#SBATCH --account=kcooper
#SBATCH --partition=standard
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=35
#SBATCH --time=6:00:00
#SBATCH --mem-per-cpu=5gb
#SBATCH --mail-type=ALL

module load anaconda/2024.06
source ~/.bashrc
micromamba activate biobakery3

# Set the directory containing the input files and the output directory
input_directory='/xdisk/kcooper/caparicio/09humann'
output_directory='/xdisk/kcooper/caparicio/tree-fruit/09humann/output'
chocophlan_database='/xdisk/kcooper/caparicio/tree-fruit/chocophlan'
uniref_database='/xdisk/kcooper/caparicio/tree-fruit/uniref'

# Check if the directories exist
if [ ! -d "$output_directory" ]; then
  mkdir -p "$output_directory"
fi

if [ ! -d "$chocophlan_database" ] || [ ! -d "$uniref_database" ]; then
  echo "Database directories do not exist: $chocophlan_database or $uniref_database"
  exit 1
fi

# Run the Python script
python3 humann_step1.py "${input_directory}" "${output_directory}" "${chocophlan_database}" "${uniref_database}"
```
### humann_step1.py

- make .py script executable
'dir="/xdisk/kcooper/caparicio/tree-fruit/09humann"'
'chmod +x "$dir"/*.py'

``` py
#!/usr/bin/env python3

import os
import sys
import subprocess
from concurrent.futures import ThreadPoolExecutor

def generate_file_list(input_directory):
    """Generate a list of .fastq files in the input directory."""
    return [os.path.join(input_directory, file) for file in os.listdir(input_directory) if file.endswith('.fastq')]

def run_humann(sample, output_dir, chocophlan_database, uniref_database):
    """Run HUMAnN for a given sample."""
    output_sample_dir = os.path.join(output_dir, os.path.basename(sample).replace('.fastq', ''))
    os.makedirs(output_sample_dir, exist_ok=True)
    command = (
        f"humann "
        f"--input {sample} "
        f"--output {output_sample_dir} "
        f"--input-format fastq "
        f"--threads 35 "
        f"--nucleotide-database {chocophlan_database} "
        f"--protein-database {uniref_database}"
    )
    try:
        subprocess.run(command, shell=True, check=True)
    except subprocess.CalledProcessError as e:
        print(f"Error processing sample {sample}: {e}")

def main(input_directory, output_directory, chocophlan_database, uniref_database):
    samples = generate_file_list(input_directory)
    with ThreadPoolExecutor(max_workers=35) as executor:
        futures = [executor.submit(run_humann, sample, output_directory, chocophlan_database, uniref_database) for sample in samples]
        for future in futures:
            try:
                future.result()
            except Exception as e:
                print(f"Error in future: {e}")

if __name__ == "__main__":
    if len(sys.argv) != 5:
        print("Usage: python humann_step1.py <input_directory> <output_directory> <chocophlan_database> <uniref_database>")
        sys.exit(1)
    input_directory = sys.argv[1]
    output_directory = sys.argv[2]
    chocophlan_database = sys.argv[3]
    uniref_database = sys.argv[4]
    main(input_directory, output_directory, chocophlan_database, uniref_database)
```
***
## humann step1 metaphlan error
```
CRITICAL ERROR: Error executing: /home/u19/member/.micromamba/envs/biobakery3/bin/metaphlan /xdisk/pi/member/tree-fruit/09humann/output/peaches264/peaches264_humann_temp/tmp932ks87e/tmpe5pl6ejs -t rel_ab -o /xdisk/pi/member/tree-fruit/09humann/output/peaches264/peaches264_humann_temp/peaches264_metaphlan_bugs_list.tsv --input_type fastq --bowtie2out /xdisk/pi/member/tree-fruit/09humann/output/peaches264/peaches264_humann_temp/peaches264_metaphlan_bowtie2.txt --nproc 35

Error message returned from metaphlan :
Error: Unable to find the mpa_pkl file at: mpa_pklExiting...

Running metaphlan ........
```
Solution:
` metaphlan databases are downloaded in uncompressed and compressed formats. Compressed databases need to be extracted:
```
tar -xf mpa_vJun23_CHOCOPhlAnSGB_202403.tar
```
***
## humann step 2 draft scripts
 
### humann_step2.slurm
``` slurm
#!/bin/bash
#SBATCH --job-name=humann_step2
#SBATCH --account=pi
#SBATCH --partition=standard
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=35
#SBATCH --time=24:00:00
#SBATCH --mem-per-cpu=5gb
#SBATCH --mail-type=ALL

module load anaconda/2024.06
source ~/.bashrc
micromamba activate biobakery3

# Set the directory containing the input files and the output directory
input_directory='/xdisk/pi/member/tree-fruit/09humann/output'
output_directory='/xdisk/pi/member/tree-fruit/09humann/output_normalized'

# Check if the output directory exists, if not, create it
if [ ! -d "$output_directory" ]; then
  mkdir -p "$output_directory"
fi

# Run the Python script
python3 humann_step2.py "${input_directory}" "${output_directory}"
```
### humann_step2.py

``` py
#!/usr/bin/env python3

import os
import sys
import subprocess
from concurrent.futures import ThreadPoolExecutor

def generate_file_list(input_directory, suffix='_genefamilies.tsv'):
    """Generate a list of files to be processed in the input directory."""
    return [os.path.join(input_directory, file) for file in os.listdir(input_directory) if file.endswith(suffix)]

def run_humann_normalization(input_file, output_dir):
    """Run HUMAnN normalization for a given file."""
    sample_name = os.path.basename(input_file).replace('_genefamilies.tsv', '')
    output_file_relab = os.path.join(output_dir, f"{sample_name}_genefamilies_relab.tsv")
    output_file_metacyc = os.path.join(output_dir, f"{sample_name}_genefamilies_relab_metacyc.tsv")
    
    commands = [
        f"humann_renorm_table --input {input_file} --output {output_file_relab} --units relab",
        f"humann_regroup_table --input {output_file_relab} --groups uniref90_to_metacyc --output {output_file_metacyc}"
    ]
    
    for command in commands:
        try:
            subprocess.run(command, shell=True, check=True)
        except subprocess.CalledProcessError as e:
            print(f"Error processing file {input_file}: {e}")

def main(input_directory, output_directory):
    files = generate_file_list(input_directory)
    with ThreadPoolExecutor(max_workers=35) as executor:
        futures = [executor.submit(run_humann_normalization, file, output_directory) for file in files]
        for future in futures:
            try:
                future.result()
            except Exception as e:
                print(f"Error in future: {e}")

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: python humann_step2.py <input_directory> <output_directory>")
        sys.exit(1)
    input_directory = sys.argv[1]
    output_directory = sys.argv[2]
    main(input_directory, output_directory)
```
***
## ghostkoala in the works
- kegg db download cost-prohibitive
- figuring out how to handle large file submissions
<a href="https://www.kegg.jp/ghostkoala/" target="_blank">GhostKOALA</a>
<a href="https://merenlab.org/2018/01/17/importing-ghostkoala-annotations/" target="_blank">Importing GhostKOALA/KEGG annotations into anvi'o</a>
<a href="https://hypocolypse.github.io/mg-workflow-1.html#ghostkoala-annotations" target="_blank">GhostKOALA Annotations</a>
***
## To do:
- humann steps 2-3
- ghostkoala
- cohesive visualizations for kraken2, abricate (which includes vfdb), humann, and ghostkoala
